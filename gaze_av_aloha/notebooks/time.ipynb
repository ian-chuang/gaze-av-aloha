{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1125554/1586919117.py:48: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize_config_dir(config_dir=config_dir, job_name=\"my_app\"):\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize_config_dir, compose\n",
    "from gaze_av_aloha.configs import Config\n",
    "from omegaconf import OmegaConf\n",
    "import gaze_av_aloha\n",
    "from gaze_av_aloha.policies.gaze_policy.gaze_policy import GazePolicy\n",
    "from gym_av_aloha.datasets.av_aloha_dataset import AVAlohaDataset, AVAlohaDatasetMeta\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# GFLOPs Calculation\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import torch\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" \n",
    "\n",
    "# Path to your config directory (adjust as needed)\n",
    "config_dir = os.path.abspath(\"../configs\")\n",
    "\n",
    "# overrides = [\n",
    "#     \"policy=vit_policy\",\n",
    "#     \"task=av_aloha_sim_thread_needle\",\n",
    "#     \"policy.visualize=False\",\n",
    "# ]\n",
    "# overrides = [\n",
    "#     \"policy=low_res_vit_policy\",\n",
    "#     \"task=av_aloha_sim_thread_needle\",\n",
    "#     \"policy.visualize=False\",\n",
    "# ]\n",
    "overrides = [\n",
    "    \"policy=foveated_vit_policy\",\n",
    "    \"task=av_aloha_sim_thread_needle\",\n",
    "    \"policy.visualize=False\",\n",
    "]\n",
    "# overrides = [\n",
    "#     \"policy=foveated_vit_policy\",\n",
    "#     \"task=av_aloha_sim_thread_needle\",\n",
    "#     \"policy.visualize=False\",\n",
    "#     \"policy.use_gaze_as_action=false\", \n",
    "#     \"policy.gaze_model_repo_id=iantc104/gaze_model_av_aloha_sim_thread_needle\",\n",
    "# ]\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "with initialize_config_dir(config_dir=config_dir, job_name=\"my_app\"):\n",
    "    cfg: Config = compose(config_name=\"default\", overrides=overrides)\n",
    "\n",
    "dataset_meta = AVAlohaDatasetMeta(repo_id=cfg.task.dataset_repo_id, root=cfg.task.dataset_root)\n",
    "policy = GazePolicy(cfg.policy, cfg.task, dataset_meta.stats).to(device)\n",
    "policy = policy.eval()\n",
    "\n",
    "dataset = AVAlohaDataset(\n",
    "    repo_id=cfg.task.dataset_repo_id,\n",
    "    delta_timestamps=policy.get_delta_timestamps(),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44de1116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 25 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::unflatten encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 60 time(s)\n",
      "Unsupported operator aten::softmax encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.0.attn.out_proj, blocks.0.drop_path, blocks.1.attn.out_proj, blocks.1.drop_path, blocks.10.attn.out_proj, blocks.10.drop_path, blocks.11.attn.out_proj, blocks.11.drop_path, blocks.2.attn.out_proj, blocks.2.drop_path, blocks.3.attn.out_proj, blocks.3.drop_path, blocks.4.attn.out_proj, blocks.4.drop_path, blocks.5.attn.out_proj, blocks.5.drop_path, blocks.6.attn.out_proj, blocks.6.drop_path, blocks.7.attn.out_proj, blocks.7.drop_path, blocks.8.attn.out_proj, blocks.8.drop_path, blocks.9.attn.out_proj, blocks.9.drop_path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 1982435328\n",
      "GFLOPs: 1.98\n",
      " inference time: 0.003605 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example model and input\n",
    "model = policy.flow.backbone.backbone\n",
    "patch_size = policy.flow.backbone.tokenizer.token_size\n",
    "x = torch.randn(1, policy.flow.backbone.get_num_tokens(*cfg.policy.input_shape, device=device), 3, patch_size, patch_size).to(device)\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "print(f\"FLOPs: {flops.total()}\")  # total in float\n",
    "print(f\"GFLOPs: {flops.total() / 1e9:.2f}\")\n",
    "\n",
    "n = 100\n",
    "with torch.inference_mode():\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(n):\n",
    "        model(x)\n",
    "    end_time = time.perf_counter()\n",
    "inference_time = (end_time - start_time) / n \n",
    "print(f\" inference time: {inference_time:.6f} seconds\")\n",
    "\n",
    "del x\n",
    "del flops\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bfe035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping episode_index due to shape mismatch\n",
      "Policy inference time: 0.087952 seconds\n",
      "Memory allocated: 671.56 MB\n",
      "Memory reserved:  986.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Inference Time Calc\n",
    "del policy\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "policy = GazePolicy(cfg.policy, cfg.task, dataset_meta.stats).to(device)\n",
    "policy = policy.eval()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        try:\n",
    "            batch[k] = v.to(device)[:, :cfg.policy.n_obs_steps]\n",
    "        except:\n",
    "            print(f\"Skipping {k} due to shape mismatch\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "policy.eval()\n",
    "n = 100\n",
    "with torch.inference_mode():\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in range(n):\n",
    "        _ = policy.generate_actions(batch)\n",
    "    end_time = time.perf_counter()\n",
    "inference_time = (end_time - start_time) / n \n",
    "print(f\"Policy inference time: {inference_time:.6f} seconds\")\n",
    "\n",
    "# Print memory used\n",
    "allocated = torch.cuda.memory_allocated() / 1024**2  # in MB\n",
    "reserved = torch.cuda.memory_reserved() / 1024**2  # in MB\n",
    "\n",
    "print(f\"Memory allocated: {allocated:.2f} MB\")\n",
    "print(f\"Memory reserved:  {reserved:.2f} MB\")\n",
    "\n",
    "del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1bdc2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy train time: 0.123751 seconds\n"
     ]
    }
   ],
   "source": [
    "# training time calc\n",
    "\n",
    "# Inference Time Calc\n",
    "del policy\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "policy = GazePolicy(cfg.policy, cfg.task, dataset_meta.stats).to(device)\n",
    "policy = policy.train()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "policy.train()\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-4)\n",
    "\n",
    "n = 100\n",
    "start_time = time.perf_counter()\n",
    "for _ in range(n):\n",
    "    optimizer.zero_grad()\n",
    "    loss, _ = policy.forward(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "end_time = time.perf_counter()\n",
    "inference_time = (end_time - start_time) / n \n",
    "print(f\"Policy train time: {inference_time:.6f} seconds\")\n",
    "\n",
    "del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49608a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15774720\n"
     ]
    }
   ],
   "source": [
    "num_total_params = sum(p.numel() for p in policy.flow.pool.parameters())\n",
    "print(num_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556109a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

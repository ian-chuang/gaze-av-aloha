<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
        content="Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers">
    <meta property="og:title" content="Look, Focus, Act" />
    <meta property="og:description"
        content="Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers" />
    <meta property="og:url" content="https://ian-chuang.github.io/gaze-av-aloha/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="Look, Focus, Act">
    <meta name="twitter:description"
        content="Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Imitation Learning, Foveated Vision, Bimanual Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>

    <!-- Navbar -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-fullhd">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Look, Focus, Act:<br />Efficient and Robust Robot
                            Learning
                            via Human Gaze and Foveated Vision Transformers</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://ian-chuang.github.io/">Ian Chuang</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://andrewcwlee.github.io/">Andrew Lee</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://www.linkedin.com/in/gao-dechen/">Dechen
                                    Gao</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="">Jinyu Zou</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank"
                                    href="https://soltanilab.engineering.ucdavis.edu/people/iman-soltani">Iman
                                    Soltani</a><sup>2</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>University of California, Berkeley </span>
                            <span class="author-block"><sup>2</sup>University of California, Davis</span>
                            <span class="author-block"><sup>3</sup>Tongji University</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <!-- Arxiv PDF link -->
                                <!-- <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2409.17435" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span> -->

                                <!-- Github link -->
                                <span class="link-block">
                                    <a href="" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (Coming Soon)</span>
                                    </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                                    <a href="https://github.com/Soltanilara/av-aloha-unity/tree/eye-tracking"
                                        target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-unity"></i>
                                        </span>
                                        <span>VR Unity Code</span>
                                    </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <!-- <span class="link-block">
                                    <a href="https://arxiv.org/abs/2409.17435" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span> -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Video Teaser -->
    <!-- <section class="hero teaser">
        <div class="container is-fullhd">
            <div class="hero-body">
                <div class="container">
                    <div class="columns is-vcentered  is-centered">
                        <div class="publication-video">
                            <iframe src="https://www.youtube.com/embed/DwJzdaKM4N0" frameborder="0"
                                allow="autoplay; encrypted-media" allowfullscreen></iframe>
                        </div>
                        </br>
                    </div>
                    <br>
                    <h2 class="subtitle has-text-centered">
                        <span class="dperact">We introduce <b>AV-ALOHA</b>, a bimanual robot system with <b>7-DoF active
                                vision</b> that is an extension of <a href="https://aloha-2.github.io/">ALOHA 2</a>.
                            This system offers an immersive teleoperation experience using VR and serves as a platform
                            to evaluate active vision in imitation learning and manipulation.</span>
                    </h2>
                </div>
            </div>
        </div>
    </section> -->

    <section class="hero teaser">
        <div class="container is-fullhd">
            <div class="hero-body">
                <div class="container">
                    <div class="columns is-vcentered  is-centered">
                        <img src="static/images/system.png" class="interpolation-image" />
                    </div>
                    <br>
                    <h2 class="subtitle has-text-centered">
                        <span class="dperact">We introduce a human-inspired <b>foveated vision framework for robot
                                learning</b>
                            that integrates human gaze with foveated Vision Transformers and robotic control, enabling
                            efficient and
                            robust policies.</span>
                    </h2>
                </div>
            </div>
        </div>
    </section>

    <!-- Abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Human vision is a highly active process driven by gaze, which directs attention and fixation
                            to task-relevant regions and dramatically reduces visual processing. In contrast, robot
                            learning systems typically rely on passive, uniform processing of raw camera images. In this
                            work, we explore how incorporating human-like active gaze into robotic policies can enhance
                            both efficiency and performance. We build on recent advances in foveated image processing
                            and apply them to an Active Vision robot system that emulates both human head movement and
                            eye tracking. Extending prior work on the AV-ALOHA robot simulation platform, we introduce a
                            framework for simultaneously collecting eye-tracking data and robot demonstrations from a
                            human operator as well as a simulation benchmark and dataset for training robot policies
                            that incorporate human gaze. Given the widespread use of Vision Transformers (ViTs) in robot
                            learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme
                            inspired by recent work in image segmentation. Compared to uniform patch tokenization, this
                            significantly reduces the number of tokens—and thus computation—without sacrificing visual
                            fidelity near regions of interest. We also explore two approaches to gaze imitation and
                            prediction from human data. The first is a structured, hierarchical two-stage model that
                            first predicts gaze, which is then used to guide foveation and action prediction. The second
                            is a novel method that treats gaze as an extension of whole-body control, integrating it
                            into the robot's action space such that the policy directly predicts both future gaze and
                            actions in an end-to-end manner. Our results show that our method for foveated robot vision
                            not only drastically reduces computational overhead, but also improves performance for high
                            precision tasks and robustness to unseen distractors. Together, these findings suggest that
                            human-inspired visual processing offers a useful inductive bias for robotic vision systems.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section hero">
        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Data Collection</h2>
                    <img src="static/images/data_collection.png" class="interpolation-image" style="max-width: 50em; width: 100%; height: auto" />
                    <br />
                    <br />

                    <div class="content has-text-justified">

                        <p>
                            We use the <b>AV-ALOHA</b> simulation platform to collect bimanual robot demonstrations with
                            human eye-tracking data. The robot streams stereo camera images to the VR headset for visual
                            feedback, while the headset transmits head and hand controller poses to control the robot,
                            along with human gaze data.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!-- Simulation Tasks -->
    <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title is-3">
                    Human Demonstrations with Eye-Tracking
                </h2>

                <!-- First Row of Simulation Videos -->
                <div class="columns is-centered">
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/cube_demo.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Cube Transfer</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/peg_demo.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Peg Insertion</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/slot_demo.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Slot Insertion</h4>
                    </div>
                </div>

                <!-- Second Row of Simulation Videos -->
                <div class="columns is-centered">
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/hook_demo.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Hook Package</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/pour_demo.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Pour Test Tube</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/thread_demo.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Thread Needle</h4>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section hero">
        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Policy Architecture</h2>
                    <img src="static/images/policy.png" class="interpolation-image" />
                    <br />
                    <br />

                    <div class="content has-text-justified">

                        <p>
                            <b>Gaze Prediction:</b> Gaze is predicted using two approaches: <i>Fov-UNet</i>, a
                            hierarchical two-stage model that first predicts gaze with a UNet and then uses it to guide
                            the policy, and <i>Fov-Act</i>, a novel end-to-end method that treats gaze as part of the
                            robot's action space where the policy predicts both gaze and action together.

                            <b>Tokenization:</b> <i>Fov-UNet</i> and <i>Fov-Act</i> methods use foveated tokenization
                            around predicted gaze. The other methods, <i>Fine</i> and <i>Coarse</i>, do not predict gaze
                            and
                            use a standard uniform tokenization.

                            <b>Policy Architecture:</b> We use a Transformer-Based Flow Matching Policy. Image
                            observations <code>O<sub>img</sub></code> are tokenized,
                            processed by ViT, and compressed with a Q-Former module into tokens
                            <code>c<sub>img</sub></code>, which condition the Flow Transformer (FT) via cross-attention.
                            Proprioception is encoded by an MLP into tokens <code>c<sub>proprio</sub></code> and added
                            to the FT input sequence. Timestep <code>t</code> is embedded and conditions FT via AdaLN.
                            FT predicts flow matching velocity <code>v<sub>&theta;</sub></code> from noisy action latent
                            <code>z<sub>t</sub></code>, <code>c<sub>img</sub></code>, <code>c<sub>proprio</sub></code>,
                            and <code>t</code>. Actions are generated via Euler integration.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section hero">
        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Foveated Tokenization</h2>
                    <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                        <source src="static/videos/thread_tokenization.mp4" type="video/mp4" />
                    </video>
                    <br />
                    <br />

                    <div class="content has-text-justified">

                        <p>
                            (Left) The input image is divided into patches using either the standard uniform tokenization
                            (Middle) or foveated tokenization (Right). Foveated tokenization mimics the human retina by
                            assigning high-resolution patches near the gaze point and lower resolution in the periphery.
                            This reduces the number of tokens from 324 (uniform) to just 20 (foveated), greatly lowering
                            computational cost while preserving detail where it matters most.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Simulation Tasks -->
    <section class="hero is-small is-light">
        <div class="hero-body">
            <div class="container">
                <h2 class="title is-3">
                    Autonomous Rollout with Foveated Vision
                </h2>

                <!-- First Row of Simulation Videos -->
                <div class="columns is-centered">
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/cube_fov.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Cube Transfer</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/peg_fov.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Peg Insertion</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/slot_fov.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Slot Insertion</h4>
                    </div>
                </div>

                <!-- Second Row of Simulation Videos -->
                <div class="columns is-centered">
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/hook_fov.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Hook Package</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/pour_fov.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Pour Test Tube</h4>
                    </div>
                    <div class="column is-one-third">
                        <video poster="" autoplay controls muted loop width="100%" style="object-fit: cover">
                            <source src="static/videos/thread_fov.mp4" type="video/mp4" />
                        </video>
                        <h4 class="subtitle has-text-centered">Thread Needle</h4>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section class="section hero">
        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results</h2>
                    <img src="static/images/success.png" class="interpolation-image" />
                    <br />
                    <br />
                    <img src="static/images/latency.png" class="interpolation-image" />
                </div>
            </div>
        </div>
    </section> -->


    <!--BibTex citation -->
    <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{chuang2024activevisionneedexploring,
    title={Active Vision Might Be All You Need: Exploring Active Vision in Bimanual Robotic Manipulation}, 
    author={Ian Chuang and Andrew Lee and Dechen Gao and Iman Soltani},
    year={2024},
    eprint={2409.17435},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/2409.17435}, 
}</code></pre>
        </div>
    </section> -->
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the source code of this website, we just ask that you link back to
                            this page in the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>



</body>

</html>
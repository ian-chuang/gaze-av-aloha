{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915542c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/gaze/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d320fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "class PatchShuffle():\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "786c6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaze_av_aloha.policies.gaze_policy.vit import named_apply, init_weights_vit_timm, extend_valid_token_mask\n",
    "from gaze_av_aloha.policies.gaze_policy.vit import Block as ViTBlock\n",
    "from typing import Optional, Type, Callable\n",
    "import torch\n",
    "\n",
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens: int,\n",
    "        num_registers: int,\n",
    "        patch_size: int,\n",
    "        depth: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        act_layer: Type[torch.nn.Module],\n",
    "        drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        mask_ratio: float = 0.75\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert num_registers == 1, \"assume only using CLS token\"\n",
    "\n",
    "        self.patch_emb = torch.nn.Linear(patch_size * patch_size * 3, embedding_dim)\n",
    "\n",
    "        self.pos_enc = torch.nn.Parameter(torch.randn(num_tokens, embedding_dim))\n",
    "\n",
    "        self.reg_tokens = torch.nn.Parameter(torch.randn(num_registers, embedding_dim))\n",
    "\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.blocks.append(\n",
    "                ViTBlock(\n",
    "                    dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    act_layer=act_layer,\n",
    "                    drop=drop,\n",
    "                    drop_path=drop_path,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @property\n",
    "    def embed_dim(self) -> int:\n",
    "        return self.patch_emb.out_features\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.trunc_normal_(self.pos_enc, std=0.02)\n",
    "        torch.nn.init.normal_(self.reg_tokens, std=1e-6)\n",
    "        named_apply(init_weights_vit_timm, self)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - foveated_tokens (B, N, E): The input foveated image tokens.\n",
    "        - valid_token_mask (B, N): An optional mask indicating which tokens are valid.\n",
    "\n",
    "        Returns:\n",
    "        - features (B, N, C): The output feature vectors, one per input token\n",
    "        - register_features (B, N, C): The register feature vectors, one per register token\n",
    "        \"\"\"\n",
    "        # normal\n",
    "        tokens = self.patch_emb(\n",
    "            tokens.flatten(2)\n",
    "        ) + self.pos_enc.unsqueeze(0) # (B, S, D)\n",
    "\n",
    "        # not normal\n",
    "        tokens = tokens.transpose(0,1) # (S, B, D)\n",
    "        tokens, forward_indexes, backward_indexes = self.shuffle.forward(tokens)\n",
    "        tokens = tokens.transpose(0,1) # (B, S, D)\n",
    "\n",
    "        # normal \n",
    "        num_registers = self.reg_tokens.shape[0]\n",
    "        tokens = torch.cat(\n",
    "            [\n",
    "                tokens,\n",
    "                self.reg_tokens.unsqueeze(0).expand(tokens.shape[0], -1, -1),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        valid_token_mask = extend_valid_token_mask(mask, num_registers)\n",
    "        invalid_token_mask = (\n",
    "            valid_token_mask.logical_not() if valid_token_mask is not None else None\n",
    "        )\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, invalid_token_mask)\n",
    "        # tokens = self.norm(tokens) \n",
    "\n",
    "        # not normal\n",
    "        tokens = tokens.transpose(0,1) # (S, B, D)\n",
    "\n",
    "        return tokens, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8dd088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaze_av_aloha.policies.gaze_policy.tokenizer import BaseImageTokenizer, FoveatedImageTokenizer, ImageTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:] # remove global feature\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "\n",
    "        return patches, mask   \n",
    "\n",
    "\n",
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 tokenizer: ImageTokenizer,\n",
    "                 encoder: MAE_Encoder,\n",
    "                 decoder: MAE_Decoder, \n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, img):\n",
    "        tokens, mask = self.tokenizer.tokenize(img)\n",
    "        features, backward_indexes = self.encoder(tokens, mask)\n",
    "        pred_tokens, mask = self.decoder(features,  backward_indexes)\n",
    "        return tokens.flatten(2).transpose(0,1), pred_tokens, mask \n",
    "\n",
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        logits = self.head(features[0])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5de9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5181e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3554, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BaseImageTokenizer(token_size=2, height=32, width=32)\n",
    "encoder = MAE_Encoder(\n",
    "    num_tokens=tokenizer.get_num_tokens(),\n",
    "    num_registers=1,\n",
    "    patch_size=2,\n",
    "    depth=12,\n",
    "    embedding_dim=192,\n",
    "    num_heads=3,\n",
    "    act_layer=torch.nn.GELU,\n",
    ")\n",
    "decoder = MAE_Decoder()\n",
    "mae = MAE_ViT(\n",
    "    tokenizer=tokenizer,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")\n",
    "\n",
    "# features, backward_indexes = encoder(tokens)\n",
    "# print(forward_indexes.shape)\n",
    "# pred_tokens, mask = decoder(features, backward_indexes)\n",
    "# print(pred_tokens.shape)\n",
    "# print(tokens.flatten(2).transpose(0,1).shape)\n",
    "img = torch.rand(64, 3, 32, 32)\n",
    "tokens, pred_tokens, mask = mae(img)\n",
    "loss = torch.mean((pred_tokens - tokens) ** 2 * mask / 0.75)\n",
    "print(loss)\n",
    "\n",
    "# features, backward_indexes = self.encoder(img)\n",
    "# predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "# return predicted_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af167d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
